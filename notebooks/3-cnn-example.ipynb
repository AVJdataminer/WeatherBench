{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Train a CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import *\n",
    "import tensorflow.keras.backend as K\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from src.score import *\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def limit_mem():\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "limit_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')\n",
    "sns.set_context('notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# DATADIR = '/mnt/netdisk1/stephan/5.625deg/'\n",
    "DATADIR = '/data/weather-benchmark/5.625deg/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Create data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Load the validation subset of the data: 2017 and 2018\n",
    "z500_valid = load_test_data(f'{DATADIR}geopotential_500', 'z')\n",
    "t850_valid = load_test_data(f'{DATADIR}temperature_850', 't')\n",
    "valid = xr.merge([z500_valid, t850_valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "z = xr.open_mfdataset(f'{DATADIR}geopotential_500/*.nc', combine='by_coords')\n",
    "t = xr.open_mfdataset(f'{DATADIR}temperature_850/*.nc', combine='by_coords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "datasets = [z, t]\n",
    "ds = xr.merge(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "ds_train = ds.sel(time=slice('2015', '2016'))\n",
    "ds_test = ds.sel(time=slice('2017', '2018'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import pdb\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    \"\"\"https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\"\"\"\n",
    "    def __init__(self, ds, var_dict, lead_time, batch_size=32, shuffle=True, load=True, mean=None, std=None):\n",
    "        \n",
    "        self.ds = ds\n",
    "        self.var_dict = var_dict\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.lead_time = lead_time\n",
    "        \n",
    "        data = []\n",
    "        generic_level = xr.DataArray([1], coords={'level': [1]}, dims=['level'])\n",
    "        for var, levels in var_dict.items():\n",
    "            try:\n",
    "                data.append(ds[var].sel(level=levels))\n",
    "            except ValueError:\n",
    "                data.append(ds[var].expand_dims({'level': generic_level}, 1))\n",
    "\n",
    "        self.data = xr.concat(data, 'level').transpose('time', 'lat', 'lon', 'level')\n",
    "        self.mean = self.data.mean(('time', 'lat', 'lon')).compute() if mean is None else mean\n",
    "        self.std = self.data.std('time').mean(('lat', 'lon')).compute() if std is None else std\n",
    "        # Normalize\n",
    "        self.data = (self.data - self.mean) / self.std\n",
    "        self.n_samples = self.data.isel(time=slice(0, -lead_time)).shape[0]\n",
    "        self.valid_time = self.data.isel(time=slice(lead_time, None)).time\n",
    "        \n",
    "        self.on_epoch_end()\n",
    "        \n",
    "        if load: print('Loading data into RAM'); self.data.load()\n",
    "        \n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.ceil(self.n_samples / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        'Generate one batch of data'\n",
    "        idxs = self.idxs[i*self.batch_size:(i+1)*self.batch_size]\n",
    "        X = self.data.isel(time=idxs).values\n",
    "        y = self.data.isel(time=idxs+self.lead_time).values\n",
    "        return X, y\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.idxs = np.arange(self.n_samples)\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dic = OrderedDict({'z': None, 't': None})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "bs=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data into RAM\n",
      "Loading data into RAM\n",
      "CPU times: user 3.67 s, sys: 3.59 s, total: 7.26 s\n",
      "Wall time: 740 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dg_train = DataGenerator(ds_train.sel(time=slice('2015', '2015')), dic, 5*24, batch_size=bs, load=True)\n",
    "dg_valid = DataGenerator(ds_train.sel(time=slice('2016', '2016')), dic, 5*24, batch_size=bs, mean=dg_train.mean, std=dg_train.std, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<xarray.DataArray 'z' (level: 2)>\n",
       " array([54124.863,   274.798], dtype=float32)\n",
       " Coordinates:\n",
       "   * level    (level) int64 850 850, <xarray.DataArray 'z' (level: 2)>\n",
       " array([1110.1985   ,    5.6419535], dtype=float32)\n",
       " Coordinates:\n",
       "   * level    (level) int64 850 850)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dg_train.mean, dg_train.std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data into RAM\n"
     ]
    }
   ],
   "source": [
    "dg_test = DataGenerator(ds_test, dic, 5*24, batch_size=bs, mean=dg_train.mean, std=dg_train.std, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Create model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "class PeriodicConv2D(tf.keras.layers.Conv2D):\n",
    "    \"\"\"Convolution with periodic padding in second spatial dimension (lon)\"\"\"\n",
    "    def __init__(self, filters, kernel_size, **kwargs):\n",
    "        assert type(kernel_size) is int, 'Periodic convolutions only works for square kernels.'\n",
    "        self.pad_width = (kernel_size - 1) // 2\n",
    "        super().__init__(filters, kernel_size, **kwargs)\n",
    "        assert self.padding == 'valid', 'Periodic convolution only works for valid padding.'\n",
    "        assert sum(self.strides) == 2, 'Periodic padding only works for stride (1, 1)'\n",
    "    \n",
    "    def _pad(self, inputs):\n",
    "        # Input: [samples, lat, lon, filters]\n",
    "        # Periodic padding in lon direction\n",
    "        inputs_padded = tf.concat(\n",
    "            [inputs[:, :, -self.pad_width:, :], inputs, inputs[:, :, :self.pad_width, :]], axis=2)\n",
    "        # Zero padding in the lat direction\n",
    "        inputs_padded = tf.pad(inputs_padded, [[0, 0], [self.pad_width, self.pad_width], [0, 0], [0, 0]])\n",
    "        return inputs_padded\n",
    "\n",
    "    def __call__(self, inputs, *args, **kwargs):\n",
    "        # Unfortunate workaround necessary for TF < 1.13\n",
    "        inputs_padded = Lambda(self._pad)(inputs)\n",
    "        return super().__call__(inputs_padded, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def build_cnn(filters, kernels, input_shape, activation='elu', dr=0):\n",
    "    \"\"\"Fully convolutional network\"\"\"\n",
    "    x = input = Input(shape=input_shape)\n",
    "    for f, k in zip(filters[:-1], kernels[:-1]):\n",
    "        x = PeriodicConv2D(f, k, activation=activation)(x)\n",
    "        if dr > 0: x = Dropout(dr)(x)\n",
    "    output = PeriodicConv2D(filters[-1], kernels[-1])(x)\n",
    "    return keras.models.Model(input, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/rasp/miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "cnn = build_cnn([32, 64, 64, 64, 2], [5, 5, 5, 5, 5], (32, 64, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "cnn.compile(keras.optimizers.Adam(1e-4), 'mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 32, 64, 2)]       0         \n",
      "_________________________________________________________________\n",
      "lambda (Lambda)              (None, 36, 68, 2)         0         \n",
      "_________________________________________________________________\n",
      "periodic_conv2d (PeriodicCon (None, 32, 64, 32)        1632      \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 36, 68, 32)        0         \n",
      "_________________________________________________________________\n",
      "periodic_conv2d_1 (PeriodicC (None, 32, 64, 64)        51264     \n",
      "_________________________________________________________________\n",
      "lambda_2 (Lambda)            (None, 36, 68, 64)        0         \n",
      "_________________________________________________________________\n",
      "periodic_conv2d_2 (PeriodicC (None, 32, 64, 64)        102464    \n",
      "_________________________________________________________________\n",
      "lambda_3 (Lambda)            (None, 36, 68, 64)        0         \n",
      "_________________________________________________________________\n",
      "periodic_conv2d_3 (PeriodicC (None, 32, 64, 64)        102464    \n",
      "_________________________________________________________________\n",
      "lambda_4 (Lambda)            (None, 36, 68, 64)        0         \n",
      "_________________________________________________________________\n",
      "periodic_conv2d_4 (PeriodicC (None, 32, 64, 2)         3202      \n",
      "=================================================================\n",
      "Total params: 261,026\n",
      "Trainable params: 261,026\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "270/270 [==============================] - 12s 46ms/step - loss: 0.8910 - val_loss: 0.6762\n",
      "Epoch 2/100\n",
      "270/270 [==============================] - 10s 36ms/step - loss: 0.6360 - val_loss: 0.6590\n",
      "Epoch 3/100\n",
      "270/270 [==============================] - 9s 35ms/step - loss: 0.6212 - val_loss: 0.6547\n",
      "Epoch 4/100\n",
      "270/270 [==============================] - 10s 36ms/step - loss: 0.6124 - val_loss: 0.6429\n",
      "Epoch 5/100\n",
      "270/270 [==============================] - 9s 35ms/step - loss: 0.6058 - val_loss: 0.6465\n",
      "Epoch 6/100\n",
      "270/270 [==============================] - 9s 35ms/step - loss: 0.6006 - val_loss: 0.6406\n",
      "Epoch 7/100\n",
      "270/270 [==============================] - 9s 34ms/step - loss: 0.5952 - val_loss: 0.6422\n",
      "Epoch 8/100\n",
      "270/270 [==============================] - 9s 35ms/step - loss: 0.5906 - val_loss: 0.6378\n",
      "Epoch 9/100\n",
      "270/270 [==============================] - 10s 36ms/step - loss: 0.5867 - val_loss: 0.6334\n",
      "Epoch 10/100\n",
      "270/270 [==============================] - 10s 36ms/step - loss: 0.5826 - val_loss: 0.6357\n",
      "Epoch 11/100\n",
      "270/270 [==============================] - 10s 35ms/step - loss: 0.5784 - val_loss: 0.6359\n",
      "Epoch 00011: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f3f6807d940>"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit_generator(dg_train, epochs=100, validation_data=dg_valid, \n",
    "                  callbacks=[tf.keras.callbacks.EarlyStopping(\n",
    "                                monitor='val_loss',\n",
    "                                min_delta=0,\n",
    "                                patience=2,\n",
    "                                verbose=1, \n",
    "                                mode='auto'\n",
    "                            )]\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "cnn.save_weights('/home/rasp/cube_home/tmp/test.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def create_predictions(model, dg):\n",
    "    preds = model.predict_generator(dg)\n",
    "    # Unnormalize\n",
    "    preds = preds * dg.std.values + dg.mean.values\n",
    "    fcs = []\n",
    "    lev_idx = 0\n",
    "    for var, levels in dg.var_dict.items():\n",
    "        if levels is None:\n",
    "            fcs.append(xr.DataArray(\n",
    "                preds[:, :, :, lev_idx],\n",
    "                dims=['time', 'lat', 'lon'],\n",
    "                coords={'time': dg.valid_time, 'lat': dg.ds.lat, 'lon': dg.ds.lon},\n",
    "                name=var\n",
    "            ))\n",
    "            lev_idx += 1\n",
    "        else:\n",
    "            nlevs = len(levels)\n",
    "            fcs.append(xr.DataArray(\n",
    "                preds[:, :, :, lev_idx:lev_idx+nlevs],\n",
    "                dims=['time', 'lat', 'lon', 'level'],\n",
    "                coords={'time': dg.valid_time, 'lat': dg.ds.lat, 'lon': dg.ds.lon, 'level': levels},\n",
    "                name=var\n",
    "            ))\n",
    "            lev_idx += nlevs\n",
    "    return xr.merge(fcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "fc = create_predictions(cnn, dg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:  ()\n",
       "Coordinates:\n",
       "    level    int32 850\n",
       "Data variables:\n",
       "    z_rmse   float64 802.9\n",
       "    t_rmse   float64 3.591"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_weighted_rmse(fc, valid).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
